{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06fb77-53a8-446b-82e1-7a1da28dac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "### **Statistics Part 2: Assignment Questions**\n",
    "\n",
    "-----\n",
    "\n",
    "#### **1. What is hypothesis testing in statistics?**\n",
    "\n",
    "Hypothesis testing is a fundamental concept in inferential statistics used to make decisions or draw conclusions about a population based on sample data. It involves setting up two competing hypotheses, the null hypothesis ($H\\_0$) and the alternative hypothesis ($H\\_a$), and then using sample evidence to determine whether there is enough evidence to reject the null hypothesis in favor of the alternative. The process involves calculating a test statistic and comparing it to a critical value or a p-value to make a decision.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **2. What is the null hypothesis, and how does it differ from the alternative hypothesis?**\n",
    "\n",
    "  * **Null Hypothesis ($H\\_0$)**: The null hypothesis is a statement of no effect, no difference, or no relationship between variables. It's the default assumption that any observed difference in the data is due to random chance. For example, a null hypothesis might state that the mean of a population is equal to a specific value.\n",
    "\n",
    "  * **Alternative Hypothesis ($H\\_a$ or $H\\_1$)**: The alternative hypothesis is a statement that contradicts the null hypothesis. It proposes that there is an effect, a difference, or a relationship. The alternative hypothesis is what the researcher is typically trying to find evidence to support. For example, an alternative hypothesis might state that the mean of a population is not equal to, greater than, or less than a specific value.\n",
    "\n",
    "The primary difference is that the **null hypothesis represents the status quo or a baseline of no change**, while the **alternative hypothesis represents the change or effect that the researcher is investigating**.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **3. What is the significance level in hypothesis testing, and why is it important?**\n",
    "\n",
    "The **significance level**, denoted by the Greek letter alpha ($\\\\alpha$), is the probability of rejecting the null hypothesis when it is actually true. In other words, it's the probability of making a Type I error. The most common significance level is 0.05 (or 5%), but other values like 0.01 and 0.10 are also used.\n",
    "\n",
    "It is important because it **sets the threshold for how strong the sample evidence must be** to reject the null hypothesis. A smaller significance level (e.g., 0.01) means that the evidence must be stronger, which reduces the chance of a Type I error but increases the chance of a Type II error (failing to reject a false null hypothesis).\n",
    "\n",
    "-----\n",
    "\n",
    "#### **4. What does a P-value represent in hypothesis testing?**\n",
    "\n",
    "The **P-value** (or probability value) represents the probability of obtaining test results at least as extreme as the results actually observed, assuming that the null hypothesis is correct. A small P-value indicates that the observed data is unlikely to have occurred if the null hypothesis were true.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **5. How do you interpret the P-value in hypothesis testing?**\n",
    "\n",
    "The interpretation of a P-value is based on comparing it to the pre-determined significance level ($\\\\alpha$):\n",
    "\n",
    "  * **If P-value â‰¤ $\\\\alpha$**: You **reject the null hypothesis**. This means there is statistically significant evidence to support the alternative hypothesis.\n",
    "  * **If P-value \\> $\\\\alpha$**: You **fail to reject the null hypothesis**. This means there is not enough statistical evidence to support the alternative hypothesis.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **6. What are Type 1 and Type 2 errors in hypothesis testing?**\n",
    "\n",
    "In hypothesis testing, two types of errors can occur:\n",
    "\n",
    "  * **Type I Error**: This occurs when you **reject a true null hypothesis**. The probability of a Type I error is equal to the significance level ($\\\\alpha$).\n",
    "  * **Type II Error**: This occurs when you **fail to reject a false null hypothesis**. The probability of a Type II error is denoted by the Greek letter beta ($\\\\beta$).\n",
    "\n",
    "| Decision | Null Hypothesis is True | Null Hypothesis is False |\n",
    "| :--- | :--- | :--- |\n",
    "| **Reject Null Hypothesis** | Type I Error | Correct Decision |\n",
    "| **Fail to Reject Null Hypothesis** | Correct Decision | Type II Error |\n",
    "\n",
    "-----\n",
    "\n",
    "#### **7. What is the difference between a one-tailed and a two-tailed test in hypothesis testing?**\n",
    "\n",
    "  * **One-Tailed Test**: A one-tailed test is used when the alternative hypothesis specifies a direction of the difference or relationship. For example, if you are testing whether a new drug *increases* a certain metric. The critical region for rejection is in only one tail of the sampling distribution.\n",
    "\n",
    "  * **Two-Tailed Test**: A two-tailed test is used when the alternative hypothesis does not specify a direction. For example, if you are testing whether a new drug has a *different* effect (either an increase or a decrease). The critical region is split between both tails of the sampling distribution.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **8. What is the Z-test, and when is it used in hypothesis testing?**\n",
    "\n",
    "The **Z-test** is a statistical test used to determine whether two population means are different when the variances are known and the sample size is large (typically n \\> 30). It is based on the standard normal distribution (Z-distribution).\n",
    "\n",
    "It is used for:\n",
    "\n",
    "  * Comparing a sample mean to a known population mean.\n",
    "  * Comparing the means of two samples.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **9. How do you calculate the Z-score, and what does it represent in hypothesis testing?**\n",
    "\n",
    "The Z-score (or Z-statistic) is calculated using the formula:\n",
    "\n",
    "$Z = \\\\frac{(\\\\bar{x} - \\\\mu)}{(\\\\frac{\\\\sigma}{\\\\sqrt{n}})}$\n",
    "\n",
    "Where:\n",
    "\n",
    "  * $\\\\bar{x}$ is the sample mean.\n",
    "  * $\\\\mu$ is the population mean.\n",
    "  * $\\\\sigma$ is the population standard deviation.\n",
    "  * $n$ is the sample size.\n",
    "\n",
    "The **Z-score represents how many standard deviations a data point (or sample mean) is from the population mean**. In hypothesis testing, it measures the strength of the evidence against the null hypothesis.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **10. What is the T-distribution, and when should it be used instead of the normal distribution?**\n",
    "\n",
    "The **T-distribution** (or Student's t-distribution) is a probability distribution that is similar to the normal distribution but has heavier tails. It is used in hypothesis testing when the **sample size is small** (typically n \\< 30) and the **population standard deviation is unknown**. As the sample size increases, the T-distribution approaches the normal distribution.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **11. What is the difference between a Z-test and a T-test?**\n",
    "\n",
    "| Feature | Z-test | T-test |\n",
    "| :--- | :--- | :--- |\n",
    "| **Population Standard Deviation** | Known | Unknown |\n",
    "| **Sample Size** | Large (n \\> 30) | Small (n \\< 30) |\n",
    "| **Distribution** | Standard Normal Distribution | T-distribution |\n",
    "\n",
    "-----\n",
    "\n",
    "#### **12. What is the T-test, and how is it used in hypothesis testing?**\n",
    "\n",
    "The **T-test** is a statistical test used to compare the means of two groups. It is used when the population standard deviation is unknown and the sample size is small.\n",
    "\n",
    "Types of T-tests:\n",
    "\n",
    "  * **One-sample T-test**: Compares the mean of a single sample to a known or hypothesized population mean.\n",
    "  * **Independent two-sample T-test**: Compares the means of two independent groups.\n",
    "  * **Paired sample T-test**: Compares the means of the same group at two different times or under two different conditions.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **13. What is the relationship between Z-test and T-test in hypothesis testing?**\n",
    "\n",
    "The T-test is closely related to the Z-test. The primary difference lies in the conditions under which they are used, mainly related to the knowledge of the population standard deviation and the sample size. As the sample size ($n$) increases, the T-distribution converges to the standard normal distribution. For large sample sizes (n \\> 30), the results of a T-test and a Z-test will be very similar.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **14. What is a confidence interval, and how is it used to interpret statistical results?**\n",
    "\n",
    "A **confidence interval** is a range of values that is likely to contain the true population parameter (e.g., the population mean) with a certain level of confidence. For example, a 95% confidence interval for a population mean implies that if you were to take many samples and construct a confidence interval for each, about 95% of those intervals would contain the true population mean.\n",
    "\n",
    "In interpreting statistical results, a confidence interval provides a range of plausible values for the population parameter. If a hypothesized value (from the null hypothesis) falls outside the confidence interval, it provides evidence against the null hypothesis.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **15. What is the margin of error, and how does it affect the confidence interval?**\n",
    "\n",
    "The **margin of error** is a statistic that expresses the amount of random sampling error in a survey's results. It is the half-width of the confidence interval. A larger margin of error means a wider confidence interval, indicating less precision in the estimate of the population parameter.\n",
    "\n",
    "The margin of error is affected by:\n",
    "\n",
    "  * **Confidence level**: Higher confidence levels lead to a larger margin of error.\n",
    "  * **Sample size**: Larger sample sizes lead to a smaller margin of error.\n",
    "  * **Sample variability**: Higher variability leads to a larger margin of error.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **16. How is Bayes' Theorem used in statistics, and what is its significance?**\n",
    "\n",
    "**Bayes' Theorem** describes the probability of an event based on prior knowledge of conditions that might be related to the event. The formula is:\n",
    "\n",
    "$P(A|B) = \\\\frac{P(B|A) \\\\cdot P(A)}{P(B)}$\n",
    "\n",
    "Where:\n",
    "\n",
    "  * $P(A|B)$ is the posterior probability (the probability of A given B).\n",
    "  * $P(B|A)$ is the likelihood (the probability of B given A).\n",
    "  * $P(A)$ is the prior probability (the initial belief in A).\n",
    "  * $P(B)$ is the marginal probability of B.\n",
    "\n",
    "Its significance lies in its application in **Bayesian inference**, a statistical method where Bayes' theorem is used to update the probability for a hypothesis as more evidence or information becomes available. It is widely used in fields like machine learning, medical diagnosis, and spam filtering.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **17. What is the Chi-square distribution, and when is it used?**\n",
    "\n",
    "The **Chi-square ($\\\\chi^2$) distribution** is a continuous probability distribution that is widely used in hypothesis testing. The shape of the distribution depends on the degrees of freedom.\n",
    "\n",
    "It is used in:\n",
    "\n",
    "  * **Goodness-of-fit tests**: To determine if a sample of data comes from a specific distribution.\n",
    "  * **Tests of independence**: To determine if there is a significant association between two categorical variables.\n",
    "  * **Tests for variance**: To test hypotheses about the variance of a population.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **18. What is the Chi-square goodness of fit test, and how is it applied?**\n",
    "\n",
    "The **Chi-square goodness-of-fit test** is used to determine whether an observed frequency distribution differs from a theoretical (expected) frequency distribution.\n",
    "\n",
    "It is applied by:\n",
    "\n",
    "1.  Stating the null and alternative hypotheses.\n",
    "2.  Calculating the expected frequencies for each category.\n",
    "3.  Calculating the Chi-square statistic using the formula: $\\\\chi^2 = \\\\sum \\\\frac{(O - E)^2}{E}$, where O is the observed frequency and E is the expected frequency.\n",
    "4.  Determining the critical value from the Chi-square distribution table based on the significance level and degrees of freedom.\n",
    "5.  Comparing the calculated Chi-square statistic to the critical value to make a decision.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **19. What is the F-distribution, and when is it used in hypothesis testing?**\n",
    "\n",
    "The **F-distribution** is a continuous probability distribution that is used in hypothesis testing, particularly in Analysis of Variance (ANOVA) and F-tests. The shape of the F-distribution depends on two degrees of freedom parameters.\n",
    "\n",
    "It is used for:\n",
    "\n",
    "  * **Comparing the variances of two populations** (F-test for equality of variances).\n",
    "  * **Comparing the means of three or more groups** (ANOVA).\n",
    "\n",
    "-----\n",
    "\n",
    "#### **20. What is an ANOVA test, and what are its assumptions?**\n",
    "\n",
    "**ANOVA (Analysis of Variance)** is a statistical test used to compare the means of two or more groups to determine if there are any statistically significant differences between them.\n",
    "\n",
    "The main assumptions of ANOVA are:\n",
    "\n",
    "  * **Independence**: The observations in each group are independent.\n",
    "  * **Normality**: The data in each group are approximately normally distributed.\n",
    "  * **Homogeneity of variances (homoscedasticity)**: The variances of the groups are equal.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **21. What are the different types of ANOVA tests?**\n",
    "\n",
    "  * **One-Way ANOVA**: Used to compare the means of three or more groups based on one independent variable (factor).\n",
    "  * **Two-Way ANOVA**: Used to study the effect of two independent variables on a dependent variable, including their interaction effect.\n",
    "  * **MANOVA (Multivariate Analysis of Variance)**: Used when there is more than one dependent variable.\n",
    "\n",
    "-----\n",
    "\n",
    "#### **22. What is the F-test, and how does it relate to hypothesis testing?**\n",
    "\n",
    "The **F-test** is a statistical test that uses the F-distribution. In the context of ANOVA, the **F-statistic** is a ratio of two variances (or mean squares). It is calculated as:\n",
    "\n",
    "$F = \\\\frac{\\\\text{Variance between groups}}{\\\\text{Variance within groups}}$\n",
    "\n",
    "In hypothesis testing, the F-test is used to:\n",
    "\n",
    "  * Test the overall significance of a regression model.\n",
    "  * Test the equality of means in ANOVA.\n",
    "  * Compare the variances of two populations.\n",
    "\n",
    "If the calculated F-statistic is larger than the critical F-value from the F-distribution table, the null hypothesis is rejected.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4337061b-37e3-4614-a418-2d02e753f830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299294b9-32c0-4463-8735-d28aad42a10a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### **Practical Python Implementations**\n",
    "\n",
    "-----\n",
    "\n",
    "#### **1. Z-test for Comparing a Sample Mean to a Known Population Mean**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "# Sample data\n",
    "data = [2.5, 3.0, 2.8, 3.2, 2.9, 3.5, 3.1, 2.7, 3.3, 2.9]\n",
    "population_mean = 3.0\n",
    "alpha = 0.05\n",
    "\n",
    "# Perform Z-test\n",
    "z_stat, p_value = ztest(data, value=population_mean)\n",
    "\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis: The sample mean is significantly different from the population mean.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: There is no significant difference.\")\n",
    "\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb2e76-932f-4508-8065-66fb832dbb29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **2. Simulate Data and Perform Hypothesis Testing**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate random data from a normal distribution\n",
    "np.random.seed(0)\n",
    "sample_data = np.random.normal(loc=105, scale=10, size=50)\n",
    "population_mean = 100\n",
    "alpha = 0.05\n",
    "\n",
    "# Perform a one-sample t-test\n",
    "t_stat, p_value = stats.ttest_1samp(sample_data, population_mean)\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48dde3e-c2dd-41c4-b2cd-04af1e53beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **3. Implement a One-Sample Z-test**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "\n",
    "def one_sample_ztest(sample_data, pop_mean, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a one-sample Z-test.\n",
    "    \"\"\"\n",
    "    z_stat, p_value = ztest(sample_data, value=pop_mean)\n",
    "    \n",
    "    print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < alpha:\n",
    "        print(\"Reject the null hypothesis.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis.\")\n",
    "\n",
    "# Example usage\n",
    "sample = np.random.normal(loc=5.2, scale=1.5, size=100)\n",
    "one_sample_ztest(sample, 5.0)\n",
    "\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d5f575-8e86-49d7-8c58-e20faa00ec94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **4. Two-Tailed Z-test with Visualization**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data\n",
    "x = np.linspace(-4, 4, 1000)\n",
    "y = stats.norm.pdf(x, 0, 1)\n",
    "\n",
    "# Z-test result\n",
    "z_stat = 2.5\n",
    "p_value = stats.norm.sf(abs(z_stat)) * 2\n",
    "alpha = 0.05\n",
    "critical_value = stats.norm.ppf(1 - alpha/2)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y, label='Standard Normal Distribution')\n",
    "plt.fill_between(x, y, where=(x > critical_value) | (x < -critical_value), color='red', alpha=0.5, label='Rejection Region')\n",
    "plt.axvline(z_stat, color='green', linestyle='--', label=f'Z-statistic = {z_stat:.2f}')\n",
    "plt.title('Two-Tailed Z-test Decision Region')\n",
    "plt.xlabel('Z-score')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "if p_value < alpha:\n",
    "    print(\"Reject the null hypothesis.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65248af-42ab-4daa-bbb1-9592b7d73760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **5. Visualize Type 1 and Type 2 Errors**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "def visualize_errors(mu0, mu1, sigma, n, alpha):\n",
    "    \"\"\"\n",
    "    Visualizes Type I and Type II errors.\n",
    "    \"\"\"\n",
    "    se = sigma / np.sqrt(n)\n",
    "    x = np.linspace(mu0 - 4*se, mu1 + 4*se, 1000)\n",
    "    \n",
    "    # Null and alternative distributions\n",
    "    null_dist = norm(mu0, se)\n",
    "    alt_dist = norm(mu1, se)\n",
    "\n",
    "    # Critical value\n",
    "    critical_value = norm.ppf(1 - alpha, loc=mu0, scale=se)\n",
    "\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    plt.plot(x, null_dist.pdf(x), label='Null Hypothesis ($H_0$)')\n",
    "    plt.plot(x, alt_dist.pdf(x), label='Alternative Hypothesis ($H_a$)')\n",
    "\n",
    "    # Type I error (alpha)\n",
    "    x_alpha = np.linspace(critical_value, mu0 + 4*se, 100)\n",
    "    plt.fill_between(x_alpha, null_dist.pdf(x_alpha), color='red', alpha=0.5, label=f'Type I Error (alpha) = {alpha:.2f}')\n",
    "\n",
    "    # Type II error (beta)\n",
    "    x_beta = np.linspace(mu1 - 4*se, critical_value, 100)\n",
    "    beta = alt_dist.cdf(critical_value)\n",
    "    plt.fill_between(x_beta, alt_dist.pdf(x_beta), color='blue', alpha=0.5, label=f'Type II Error (beta) = {beta:.2f}')\n",
    "    \n",
    "    plt.axvline(critical_value, color='black', linestyle='--')\n",
    "    plt.title('Type I and Type II Errors')\n",
    "    plt.xlabel('Sample Mean')\n",
    "    plt.ylabel('Probability Density')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Parameters\n",
    "visualize_errors(mu0=100, mu1=105, sigma=15, n=30, alpha=0.05)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e559a2c-49f4-406e-83b0-40ea2661f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **6. Independent T-test**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sample data for two independent groups\n",
    "group1 = np.random.normal(loc=10, scale=2, size=30)\n",
    "group2 = np.random.normal(loc=12, scale=2, size=30)\n",
    "\n",
    "# Perform independent t-test\n",
    "t_stat, p_value = stats.ttest_ind(group1, group2)\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: The means of the two groups are significantly different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24a8e94-3583-4f8f-8d5b-84d5aeecedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **7. Paired Sample T-test with Visualization**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Paired sample data (e.g., before and after treatment)\n",
    "before = np.random.normal(loc=80, scale=10, size=25)\n",
    "after = before + np.random.normal(loc=5, scale=3, size=25)\n",
    "\n",
    "# Perform paired t-test\n",
    "t_stat, p_value = stats.ttest_rel(after, before)\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(before, after)\n",
    "plt.plot([min(before), max(before)], [min(before), max(before)], color='red', linestyle='--')\n",
    "plt.title('Paired Sample T-test: Before vs. After')\n",
    "plt.xlabel('Before Treatment')\n",
    "plt.ylabel('After Treatment')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970925d7-976a-42ea-85f7-1f13261ddf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **8. Compare Z-test and T-test**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from statsmodels.stats.weightstats import ztest\n",
    "from scipy import stats\n",
    "\n",
    "# Simulate data\n",
    "sample = np.random.normal(loc=15.5, scale=2.5, size=50)\n",
    "pop_mean = 15.0\n",
    "pop_std_dev = 2.5 # Assumed for Z-test\n",
    "\n",
    "# Perform Z-test\n",
    "z_stat, z_p_value = ztest(sample, value=pop_mean, ddof=0)\n",
    "\n",
    "# Perform T-test\n",
    "t_stat, t_p_value = stats.ttest_1samp(sample, pop_mean)\n",
    "\n",
    "print(f\"Z-test: Z-statistic = {z_stat:.4f}, P-value = {z_p_value:.4f}\")\n",
    "print(f\"T-test: T-statistic = {t_stat:.4f}, P-value = {t_p_value:.4f}\")\n",
    "print(\"\\nFor larger sample sizes, the Z-test and T-test results are very similar.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734d712d-8cbc-41d0-8fbd-404bb75a8ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **9. Calculate Confidence Interval for a Sample Mean**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def confidence_interval_mean(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculates the confidence interval for a sample mean.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    se = stats.sem(data) # Standard error of the mean\n",
    "    \n",
    "    ci = stats.t.interval(confidence, df=n-1, loc=mean, scale=se)\n",
    "    \n",
    "    print(f\"{confidence*100}% Confidence Interval: {ci}\")\n",
    "    print(\"This means we are \" + str(confidence*100) + \"% confident that the true population mean lies within this interval.\")\n",
    "\n",
    "# Example usage\n",
    "sample = np.random.normal(loc=20, scale=5, size=100)\n",
    "confidence_interval_mean(sample)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31446b7b-1de7-499c-8b07-ae7f0de256be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **10. Calculate Margin of Error**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def margin_of_error(data, confidence=0.95):\n",
    "    \"\"\"\n",
    "    Calculates the margin of error for a given confidence level.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    se = stats.sem(data)\n",
    "    \n",
    "    moe = se * stats.t.ppf((1 + confidence) / 2., n-1)\n",
    "    \n",
    "    print(f\"Margin of Error: {moe:.4f}\")\n",
    "    return moe\n",
    "\n",
    "# Example usage\n",
    "sample = np.random.normal(loc=50, scale=10, size=40)\n",
    "moe = margin_of_error(sample)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599fbf03-6570-4e29-ae24-f8435fce76f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **11. Bayesian Inference with Bayes' Theorem**\n",
    "\n",
    "```python\n",
    "def bayesian_inference(prior, likelihood, evidence):\n",
    "    \"\"\"\n",
    "    Simple implementation of Bayes' Theorem.\n",
    "    \"\"\"\n",
    "    posterior = (likelihood * prior) / evidence\n",
    "    return posterior\n",
    "\n",
    "# Example: Medical Diagnosis\n",
    "# P(A) = Prior probability of having the disease\n",
    "# P(B|A) = Probability of testing positive given the disease (sensitivity)\n",
    "# P(B) = Overall probability of testing positive\n",
    "\n",
    "p_disease = 0.01\n",
    "p_positive_given_disease = 0.99 # Sensitivity\n",
    "p_positive_given_no_disease = 0.05 # False positive rate\n",
    "p_no_disease = 1 - p_disease\n",
    "\n",
    "# P(B) = P(B|A)P(A) + P(B|not A)P(not A)\n",
    "p_positive = (p_positive_given_disease * p_disease) + (p_positive_given_no_disease * p_no_disease)\n",
    "\n",
    "# Calculate P(Disease | Positive Test)\n",
    "p_disease_given_positive = bayesian_inference(p_disease, p_positive_given_disease, p_positive)\n",
    "\n",
    "print(f\"Posterior probability of having the disease given a positive test: {p_disease_given_positive:.4f}\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "685a3661-a02a-4cae-ae3a-d47b99e63b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **12. Chi-square Test for Independence**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Observed data in a contingency table\n",
    "# Rows: Smoker, Non-smoker; Columns: Lung Cancer, No Lung Cancer\n",
    "observed = np.array([[50, 10], [20, 120]])\n",
    "\n",
    "chi2, p, dof, expected = chi2_contingency(observed)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "print(f\"Degrees of freedom: {dof}\")\n",
    "print(\"Expected frequencies:\\n\", expected)\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"\\nReject the null hypothesis: There is a significant association between smoking and lung cancer.\")\n",
    "else:\n",
    "    print(\"\\nFail to reject the null hypothesis.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a17a21-4c17-467a-8e04-d846322e9f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **13. Calculate Expected Frequencies for Chi-square Test**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def expected_frequencies(observed_table):\n",
    "    \"\"\"\n",
    "    Calculates expected frequencies for a Chi-square test.\n",
    "    \"\"\"\n",
    "    row_totals = observed_table.sum(axis=1)\n",
    "    col_totals = observed_table.sum(axis=0)\n",
    "    grand_total = observed_table.sum()\n",
    "    \n",
    "    expected_table = np.outer(row_totals, col_totals) / grand_total\n",
    "    return expected_table\n",
    "\n",
    "# Example usage\n",
    "observed = np.array([[30, 10], [15, 25]])\n",
    "expected = expected_frequencies(observed)\n",
    "print(\"Observed Frequencies:\\n\", observed)\n",
    "print(\"\\nExpected Frequencies:\\n\", expected)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d9fb76-7ace-4e18-b8ea-dc038dd2a460",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **14. Goodness-of-Fit Test**\n",
    "\n",
    "```python\n",
    "from scipy.stats import chisquare\n",
    "\n",
    "# Observed frequencies of dice rolls\n",
    "observed = [9, 11, 8, 12, 10, 10] # Total 60 rolls\n",
    "# Expected frequencies for a fair die\n",
    "expected = [10, 10, 10, 10, 10, 10]\n",
    "\n",
    "chi2, p = chisquare(f_obs=observed, f_exp=expected)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2:.4f}\")\n",
    "print(f\"P-value: {p:.4f}\")\n",
    "\n",
    "if p < 0.05:\n",
    "    print(\"Reject the null hypothesis: The die is likely not fair.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The die appears to be fair.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d30bae-1e58-47cc-9185-e4e6d52228a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **15. Visualize Chi-square Distribution**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import chi2\n",
    "\n",
    "# Degrees of freedom\n",
    "df_values = [2, 5, 10, 20]\n",
    "x = np.linspace(0, 30, 1000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for df in df_values:\n",
    "    plt.plot(x, chi2.pdf(x, df), label=f'df = {df}')\n",
    "\n",
    "plt.title('Chi-square Distribution')\n",
    "plt.xlabel('Chi-square value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.ylim(0, 0.5)\n",
    "plt.show()\n",
    "print(\"Characteristics: The Chi-square distribution is right-skewed and its shape depends on the degrees of freedom. As df increases, it approaches a normal distribution.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a819a3a0-ae07-47db-9445-ecd98628ff43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **16. F-test for Comparing Variances**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import f\n",
    "\n",
    "def f_test_variances(sample1, sample2):\n",
    "    \"\"\"\n",
    "    Performs an F-test to compare the variances of two samples.\n",
    "    \"\"\"\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    n1, n2 = len(sample1), len(sample2)\n",
    "    \n",
    "    f_stat = var1 / var2\n",
    "    df1, df2 = n1 - 1, n2 - 1\n",
    "    \n",
    "    p_value = f.cdf(f_stat, df1, df2)\n",
    "    \n",
    "    print(f\"F-statistic: {f_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Example usage\n",
    "groupA = np.random.normal(loc=10, scale=3, size=20)\n",
    "groupB = np.random.normal(loc=10, scale=5, size=20)\n",
    "f_test_variances(groupA, groupB)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd176a1f-fc0b-4fd9-ab70-fe2678970187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **17. ANOVA Test**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data for three groups\n",
    "data = {'values': np.concatenate([np.random.normal(10, 2, 20),\n",
    "                                  np.random.normal(15, 2, 20),\n",
    "                                  np.random.normal(12, 2, 20)]),\n",
    "        'group': ['A']*20 + ['B']*20 + ['C']*20}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit ANOVA model\n",
    "model = ols('values ~ group', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "print(\"\\nInterpretation: Look at the P-value (PR(>F)). If it's less than your significance level, it means there is a significant difference between the means of the groups.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca31b432-d9d6-4143-b923-8d4d150a43c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **18. One-Way ANOVA with Plot**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Sample data\n",
    "group1 = np.random.normal(20, 5, 30)\n",
    "group2 = np.random.normal(25, 5, 30)\n",
    "group3 = np.random.normal(22, 5, 30)\n",
    "\n",
    "f_stat, p_value = f_oneway(group1, group2, group3)\n",
    "print(f\"F-statistic: {f_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "# Plotting\n",
    "df = pd.DataFrame({'Group1': group1, 'Group2': group2, 'Group3': group3})\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(data=df)\n",
    "plt.title('One-Way ANOVA: Comparison of Group Means')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e660af17-9ec8-4c48-b974-828d940a2776",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **19. Check ANOVA Assumptions**\n",
    "\n",
    "```python\n",
    "from scipy.stats import shapiro, levene\n",
    "\n",
    "def check_anova_assumptions(data, group_col, value_col):\n",
    "    \"\"\"\n",
    "    Checks normality and homogeneity of variances.\n",
    "    \"\"\"\n",
    "    groups = data[group_col].unique()\n",
    "    \n",
    "    # Normality (Shapiro-Wilk test)\n",
    "    print(\"Normality Check (Shapiro-Wilk):\")\n",
    "    for group in groups:\n",
    "        stat, p = shapiro(data[data[group_col] == group][value_col])\n",
    "        print(f\"Group {group}: p-value = {p:.4f}\")\n",
    "        if p < 0.05: print(\"  -> Not normal\")\n",
    "\n",
    "    # Homogeneity of variances (Levene's test)\n",
    "    print(\"\\nHomogeneity of Variances (Levene's test):\")\n",
    "    samples = [data[data[group_col] == group][value_col] for group in groups]\n",
    "    stat, p = levene(*samples)\n",
    "    print(f\"p-value = {p:.4f}\")\n",
    "    if p < 0.05: print(\"  -> Variances are not equal\")\n",
    "\n",
    "# Example using data from Q17\n",
    "check_anova_assumptions(df, 'group', 'values')\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246f5209-a646-4b0c-8f96-c9cf35b8392b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **20. Two-Way ANOVA**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Sample data with two factors\n",
    "data = {'factor1': ['A']*20 + ['B']*20 + ['A']*20 + ['B']*20,\n",
    "        'factor2': ['X']*40 + ['Y']*40,\n",
    "        'value': np.concatenate([np.random.normal(10, 2, 20),\n",
    "                                 np.random.normal(15, 2, 20),\n",
    "                                 np.random.normal(12, 2, 20),\n",
    "                                 np.random.normal(18, 2, 20)])}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Fit Two-Way ANOVA model\n",
    "model = ols('value ~ C(factor1) + C(factor2) + C(factor1):C(factor2)', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)\n",
    "\n",
    "# Visualize interaction\n",
    "sns.pointplot(data=df, x='factor1', y='value', hue='factor2', dodge=True)\n",
    "plt.title('Interaction Plot for Two-Way ANOVA')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2366691-606e-45b1-89ba-8365f3924b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **21. Visualize F-distribution**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f\n",
    "\n",
    "dfn_values = [5, 10, 20]\n",
    "dfd = 10\n",
    "x = np.linspace(0, 5, 1000)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "for dfn in dfn_values:\n",
    "    plt.plot(x, f.pdf(x, dfn, dfd), label=f'dfn={dfn}, dfd={dfd}')\n",
    "\n",
    "plt.title('F-distribution')\n",
    "plt.xlabel('F-value')\n",
    "plt.ylabel('Probability Density')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "print(\"Use in hypothesis testing: The F-distribution is used to find the P-value in ANOVA and other F-tests, helping to determine if the null hypothesis should be rejected.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525050c8-5f93-4ea4-93a3-71b8057658d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **22. One-Way ANOVA with Boxplots**\n",
    "\n",
    "(This is similar to Q18, but a more explicit example is provided here)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Data for different groups\n",
    "data = {'A': np.random.normal(100, 15, 50),\n",
    "        'B': np.random.normal(110, 15, 50),\n",
    "        'C': np.random.normal(105, 15, 50)}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "f_stat, p_value = f_oneway(df['A'], df['B'], df['C'])\n",
    "print(f\"F-statistic: {f_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "# Visualize with boxplots\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.boxplot(data=df)\n",
    "plt.title('Group Means Comparison using Boxplots')\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Group')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5061a26-e252-4abb-8ada-7ad821879394",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **23. Simulate Normal Data and Test Means**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import ttest_1samp\n",
    "\n",
    "# Simulate data from a normal distribution\n",
    "np.random.seed(42)\n",
    "simulated_data = np.random.normal(loc=50, scale=10, size=100)\n",
    "hypothesized_mean = 52\n",
    "\n",
    "# Perform hypothesis test (one-sample t-test)\n",
    "t_stat, p_value = ttest_1samp(simulated_data, hypothesized_mean)\n",
    "\n",
    "print(f\"T-statistic: {t_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(f\"Reject the null hypothesis: The mean of the simulated data is significantly different from {hypothesized_mean}.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d6aaa49-36dc-42d9-adf1-0effdecfd314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **24. Hypothesis Test for Population Variance**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import chi2\n",
    "\n",
    "def test_population_variance(sample_data, pop_variance, alpha=0.05):\n",
    "    \"\"\"\n",
    "    Performs a Chi-square test for population variance.\n",
    "    \"\"\"\n",
    "    n = len(sample_data)\n",
    "    sample_var = np.var(sample_data, ddof=1)\n",
    "    \n",
    "    chi2_stat = (n - 1) * sample_var / pop_variance\n",
    "    \n",
    "    # Two-tailed test\n",
    "    p_value = 2 * min(chi2.cdf(chi2_stat, n - 1), 1 - chi2.cdf(chi2_stat, n - 1))\n",
    "\n",
    "    print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "    print(f\"P-value: {p_value:.4f}\")\n",
    "    \n",
    "    if p_value < alpha:\n",
    "        print(f\"Reject the null hypothesis: The sample variance is significantly different from {pop_variance}.\")\n",
    "    else:\n",
    "        print(\"Fail to reject the null hypothesis.\")\n",
    "\n",
    "# Example\n",
    "data = np.random.normal(loc=0, scale=5, size=30)\n",
    "test_population_variance(data, pop_variance=25) # 5^2\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b98c00-6df4-411f-8c56-37738733b920",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **25. Z-test for Comparing Proportions**\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.proportion import proportions_ztest\n",
    "\n",
    "# Data: number of successes and number of observations\n",
    "count = np.array([50, 80]) # Successes in group 1 and 2\n",
    "nobs = np.array([100, 120]) # Total observations in group 1 and 2\n",
    "\n",
    "z_stat, p_value = proportions_ztest(count, nobs)\n",
    "\n",
    "print(f\"Z-statistic: {z_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: The proportions are significantly different.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis.\")\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7bd44e-5cfa-40bc-bb65-3c4fd9bd32b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **26. F-test for Comparing Variances with Visualization**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import f, probplot\n",
    "\n",
    "def f_test_and_visualize(sample1, sample2):\n",
    "    var1, var2 = np.var(sample1, ddof=1), np.var(sample2, ddof=1)\n",
    "    f_stat = var1 / var2 if var1 > var2 else var2 / var1\n",
    "    df1, df2 = len(sample1)-1, len(sample2)-1 if var1 > var2 else (len(sample2)-1, len(sample1)-1)\n",
    "    \n",
    "    p_value = 1 - f.cdf(f_stat, df1, df2)\n",
    "    print(f\"F-statistic: {f_stat:.4f}, P-value: {p_value:.4f}\")\n",
    "\n",
    "    # Visualization (Q-Q plots)\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    probplot(sample1, dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot for Sample 1')\n",
    "    plt.subplot(1, 2, 2)\n",
    "    probplot(sample2, dist=\"norm\", plot=plt)\n",
    "    plt.title('Q-Q Plot for Sample 2')\n",
    "    plt.show()\n",
    "\n",
    "# Example\n",
    "s1 = np.random.normal(10, 2, 50)\n",
    "s2 = np.random.normal(10, 3, 50)\n",
    "f_test_and_visualize(s1, s2)\n",
    "```\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ef71a6-8bb5-4522-b3d7-7db6ce55dd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### **27. Chi-square Goodness of Fit with Simulated Data**\n",
    "\n",
    "```python\n",
    "from scipy.stats import chisquare, norm\n",
    "import numpy as np\n",
    "\n",
    "# Simulate data from a normal distribution\n",
    "np.random.seed(1)\n",
    "sim_data = np.random.normal(loc=10, scale=2, size=100)\n",
    "\n",
    "# Create bins and get observed frequencies\n",
    "observed_freq, bins = np.histogram(sim_data, bins=10)\n",
    "\n",
    "# Get expected frequencies from a normal distribution\n",
    "expected_prob = norm.cdf(bins[1:], loc=10, scale=2) - norm.cdf(bins[:-1], loc=10, scale=2)\n",
    "expected_freq = expected_prob * 100\n",
    "\n",
    "chi2_stat, p_value = chisquare(f_obs=observed_freq, f_exp=expected_freq)\n",
    "\n",
    "print(f\"Chi-square statistic: {chi2_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Reject the null hypothesis: The data does not follow a normal distribution.\")\n",
    "else:\n",
    "    print(\"Fail to reject the null hypothesis: The data appears to follow a normal distribution.\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
